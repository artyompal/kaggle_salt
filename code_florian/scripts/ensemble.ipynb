{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBM_DIR = '../data/predicts_from_artyom'\n",
    "\n",
    "CHECKPOINT_PATH = '../model_weights/best_ensemble.pth'\n",
    "SUBM_NAME = '../subm/ensemble.csv'\n",
    "GPU = \"cuda:0\"\n",
    "\n",
    "NUM_MODELS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, math, sys, os, random, time\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from utils.data_loaders import get_data_loaders\n",
    "from utils.evaluations import FocalLoss2d, DiceLoss, get_iou_vector\n",
    "import utils.lovasz_losses as L\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, shape=(101, 101)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    if str(mask_rle) != str(np.nan):\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def rle_encode(im):\n",
    "    '''\n",
    "    im: numpy array, 1-mask, 0-background\n",
    "    Returns run length as string\n",
    "    '''\n",
    "    pixels = im.flatten(order='F')\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читает сабмит, сортирует, декодит. Возвращает словарь - id: mask\n",
    "def read_and_decode(f_path):\n",
    "    preds_df = pd.read_csv(f_path).sort_values('id')\n",
    "    ids = []\n",
    "    preds = {}\n",
    "    for idx, row in preds_df.iterrows():\n",
    "        preds[row['id']] = rle_decode(row['rle_mask'])\n",
    "    assert len(preds_df) == len(preds)\n",
    "    return preds\n",
    "\n",
    "# Собираю информацию по всем предиктам в папке - скор, фолд, путь к файлу test/train\n",
    "best_preds = {}\n",
    "for f in glob.glob(SUBM_DIR + '/*'):\n",
    "    f_name = f.split('/')[-1]\n",
    "    if 'test' in f_name:\n",
    "        test_f = f\n",
    "        train_f = f.replace('test', 'train')\n",
    "    else:\n",
    "        test_f = f.replace('train', 'test')\n",
    "        train_f = f\n",
    "    score = float(f_name.split('_')[0].replace('loc', ''))\n",
    "    fold = int(f_name.split('-')[-1].replace('.csv', ''))\n",
    "    if fold in best_preds:\n",
    "        best_preds[fold].append((score, train_f, test_f))\n",
    "    else:\n",
    "        best_preds[fold] = [(score, train_f, test_f)]\n",
    "\n",
    "# И сразу подгружаю трейн (около 800) и преобразую в словарь чтоб удобней было дальше работать\n",
    "for fold, v in tqdm_notebook(best_preds.items()):\n",
    "    print(\"Loading and decoding fold\", fold, 'with', len(v), 'snapshots')\n",
    "    decoded = []\n",
    "    for vals in sorted(v, key=lambda it: it[0]):\n",
    "        decoded.append({ 'score': vals[0],\n",
    "                        'train': read_and_decode(vals[1]),\n",
    "                        'train_f': vals[1],\n",
    "                        # Не хватает памяти загрузить сразу и тест для каждого снепшота\n",
    "                        #'test': read_and_decode(vals[2]),\n",
    "                        'test_f': vals[2] })\n",
    "    best_preds[fold] = decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чото я решил сделать такой адок – чтоб получить настоящие маски использовал и поитерировал по\n",
    "# валидационному лоадеру для каждого из фолдов -_-\n",
    "true_masks = {}\n",
    "for fold in range(5):\n",
    "    _, valid = get_data_loaders(fold=fold)\n",
    "    for batch in valid:\n",
    "        for idx in range(len(batch['id'])):\n",
    "            true_masks[batch['id'][idx]] = batch['msk'].cpu().numpy()[idx][:,13:114,13:114]\n",
    "\n",
    "assert len(true_masks) == 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вот тут я думаю и есть основной проеб. Я так и не понял как связать\n",
    "# между собой 20% предикта на трейне и все 18к предиктов на тесте.\n",
    "# Здесь я просто иду по собраной и отсортированной по скору информации\n",
    "# и собираю \"модельки\" из последних (то есть лучших по скору)\n",
    "# предиктов на трейне. А вот с тестом я ничо не понял чо делать - \n",
    "# они же полные, по 18к. Поэтому я не нашел ничо лучше чем просто\n",
    "# их усреднить – думаю проблема как раз тут.\n",
    "def compile_model(best_preds):\n",
    "    model = {'full':{}}\n",
    "    for fold in range(5):\n",
    "        snap = len(best_preds[fold]) - 1\n",
    "        if len(best_preds[fold]) == 1:\n",
    "            model[fold] = best_preds[fold][snap]\n",
    "        else:\n",
    "            model[fold] = best_preds[fold].pop(snap)\n",
    "        model['full'].update(model[fold]['train'])\n",
    "        test = read_and_decode(model[fold]['test_f'])\n",
    "        if 'test_full' not in model:\n",
    "            model['test_full'] = test\n",
    "        else:\n",
    "            for k, v in test.items():\n",
    "                model['test_full'][k] = (model['test_full'][k] + test[k])/2.\n",
    "    assert len(model['full']) == 4000\n",
    "    assert len(model['test_full']) == 18000\n",
    "    return model\n",
    "\n",
    "models = []\n",
    "\n",
    "for n in range(NUM_MODELS):\n",
    "    print('Compile model', n)\n",
    "    models.append(compile_model(best_preds))\n",
    "assert models[0]['full'].keys() == models[1]['full'].keys()\n",
    "assert models[0]['test_full'].keys() == models[1]['test_full'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не успел (и думаю что из-за ошибки логики сейчас это не должно сильно влиять)\n",
    "# поиграться с лоссом. В итоге тренил ловашом\n",
    "def train(net, optimizer, train_loader, use_lovasz=False):\n",
    "    iter_loss = 0.\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        preds = data['preds'].to(device)\n",
    "        msks = data['msks'].to(device)\n",
    "        msk_preds = net(preds)\n",
    "        if use_lovasz:\n",
    "            loss = L.lovasz_hinge(msk_preds, msks)\n",
    "        else:\n",
    "            loss = bce(msk_preds, msks)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_loss += loss.item()\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('B: {:>3}/{:<3} | {:.4}'.format(i+1,\n",
    "                                            len(train_loader),\n",
    "                                            loss.item()))\n",
    "\n",
    "    epoch_loss = iter_loss / (len(train_loader) / batch_size)\n",
    "    print('\\n' + 'Avg Train Loss: {:.4}'.format(epoch_loss))\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "def valid(net, optimizer, valid_loader, use_lovasz=False):\n",
    "    net.eval()\n",
    "    val_ious = []\n",
    "    val_iter_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            preds = data['preds'].to(device)\n",
    "            valid_msks = data['msks'].to(device)\n",
    "            msk_vpreds = net(preds)\n",
    "\n",
    "            if use_lovasz:\n",
    "                vloss = L.lovasz_hinge(msk_vpreds, valid_msks)\n",
    "            else:\n",
    "                vloss = bce(msk_vpreds, valid_msks)\n",
    "\n",
    "            val_iter_loss += vloss.item()\n",
    "            val_ious.append(get_iou_vector(valid_msks.cpu().numpy(),\n",
    "                                           msk_vpreds.sigmoid().cpu().numpy()))\n",
    "\n",
    "    epoch_vloss = val_iter_loss / (len(valid_loader) / batch_size)\n",
    "    print('Avg Eval Loss: {:.4}, Avg IOU: {:.4}'.format(epoch_vloss, np.mean(val_ious)))\n",
    "    return epoch_vloss, np.mean(val_ious)\n",
    "\n",
    "def write_csv(filename, ids, rles):\n",
    "    subm = pd.DataFrame.from_dict({'id':ids, 'rle_mask':rles}, orient='index').T\n",
    "\n",
    "    subm.to_csv(filename, index=False)\n",
    "\n",
    "    subm.index.names = ['id']\n",
    "    subm.columns = ['id', 'rle_mask']\n",
    "    print(subm.head())\n",
    "\n",
    "def predict(net, test_loader, threshold=0.5):\n",
    "    net.eval()\n",
    "\n",
    "    all_predicts = []\n",
    "    all_masks = []\n",
    "    rles = []\n",
    "    ids = []\n",
    "\n",
    "    # no gradients during validation\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(test_loader)):\n",
    "            preds = data['preds'].to(device)\n",
    "            test_ids = data['id']\n",
    "            preds = net(preds)\n",
    "\n",
    "            preds = preds.sigmoid()\n",
    "\n",
    "            pred_np = preds.squeeze().data.cpu().numpy()\n",
    "\n",
    "            for j in range(pred_np.shape[0]):\n",
    "                predicted_mask = pred_np[j]\n",
    "\n",
    "                ids.append(test_ids[j])\n",
    "\n",
    "                predicted_mask = np.where(predicted_mask > threshold, 1, 0)\n",
    "                rles.append(rle_encode(predicted_mask.astype(np.int32)))\n",
    "\n",
    "    return (ids, rles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, train_loader, valid_loader):\n",
    "    try:\n",
    "        # define optimizer\n",
    "        #optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "        # С адамом как-то красивей получилось\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        valid_ious = []\n",
    "\n",
    "        valid_patience = 0\n",
    "        best_val_metric = 1000.0\n",
    "        best_val_iou = 0.0\n",
    "        use_lovasz = True\n",
    "\n",
    "        print('Training ...')\n",
    "        for e in range(epochs):\n",
    "            print('\\n' + 'Epoch {}/{}'.format(e, epochs))\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            t_l = train(net, optimizer, train_loader, use_lovasz)\n",
    "            v_l, viou = valid(net, optimizer, valid_loader, use_lovasz)\n",
    "\n",
    "            if viou > best_val_iou:\n",
    "                net.eval()\n",
    "                torch.save(net.state_dict(), CHECKPOINT_PATH)\n",
    "                best_val_metric = v_l\n",
    "                best_val_iou = viou\n",
    "                valid_patience = 0\n",
    "            else:\n",
    "                valid_patience += 1\n",
    "\n",
    "            train_losses.append(t_l)\n",
    "            valid_losses.append(v_l)\n",
    "            valid_ious.append(viou)\n",
    "\n",
    "            print('Time: {}'.format(time.time()-start))\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    net.load_state_dict(torch.load(CHECKPOINT_PATH))\n",
    "    net.eval()\n",
    "\n",
    "    return (best_val_iou, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В датасете я собираю со всех моделек по предикту для картинки\n",
    "# сую их в NUM_MODELS слоев\n",
    "class EnsembleDataset(data.Dataset):\n",
    "    def __init__(self, models, true_masks, im_ids, valid=False):\n",
    "        self.models = models\n",
    "        self.n_models = len(models)\n",
    "        self.true_masks = true_masks\n",
    "        self.im_ids = im_ids\n",
    "        self.valid = valid\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im_id = self.im_ids[index]\n",
    "        true_mask = self.true_masks[im_id]\n",
    "        preds = []\n",
    "        for n in range(self.n_models):\n",
    "            preds.append(self.models[n]['full'][im_id])\n",
    "        preds = np.array(preds)\n",
    "        return {'id': im_id, 'msks': true_mask, 'preds': preds}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.im_ids)\n",
    "\n",
    "class EnsembleTestDataset(data.Dataset):\n",
    "    def __init__(self, models, im_ids):\n",
    "        self.models = models\n",
    "        self.n_models = len(models)\n",
    "        self.im_ids = im_ids\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im_id = self.im_ids[index]\n",
    "        preds = []\n",
    "        for n in range(self.n_models):\n",
    "            preds.append(self.models[n]['test_full'][im_id])\n",
    "        preds = np.array(preds)\n",
    "        return {'id': im_id, 'preds': preds}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.im_ids)\n",
    "\n",
    "class Conv2dBnRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3,3)):\n",
    "        super(Conv2dBnRelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "# Дропаут кстати не юзал\n",
    "class StackingFCN(nn.Module):\n",
    "    def __init__(self, input_model_nr, num_classes, filter_nr=32, dropout_2d=0.0):\n",
    "        super().__init__()\n",
    "        self.dropout_2d = dropout_2d\n",
    "\n",
    "        self.conv = nn.Sequential(Conv2dBnRelu(input_model_nr, filter_nr, kernel_size=(3, 3)),)\n",
    "        self.final = nn.Sequential(nn.Conv2d(filter_nr, num_classes, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = F.dropout2d(self.conv(x), p=self.dropout_2d)\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Спличу для тренировки трейн сет\n",
    "im_ids = list(models[0]['full'])\n",
    "train_ids, val_ids = train_test_split(im_ids, test_size=0.2)\n",
    "\n",
    "train_dataset = EnsembleDataset(models=models, true_masks=true_masks, im_ids=train_ids)\n",
    "valid_dataset = EnsembleDataset(models=models, true_masks=true_masks, im_ids=val_ids, valid=True)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "device = torch.device(GPU)\n",
    "net = StackingFCN(input_model_nr=5, num_classes=1)\n",
    "net.to(device)\n",
    "net.train()\n",
    "\n",
    "best_iou, net = train_network(net, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loaded model with best IOU\", best_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im_ids = list(models[0]['test_full'])\n",
    "test_dataset = EnsembleTestDataset(models=models, im_ids=test_im_ids)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True)\n",
    "\n",
    "ids, rles = predict(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(SUBM_NAME, ids, rles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
