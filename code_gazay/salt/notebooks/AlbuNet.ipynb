{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision %.5g\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "INFERENCE_ONLY = False#True\n",
    "\n",
    "import os\n",
    "root = os.environ['HOME'] + '/'\n",
    "competition_name = \"tgs-salt-identification-challenge\"\n",
    "short_comp_name = \"salt\"\n",
    "DATASET_PATH     = root + \"datasets/competitions/\" + competition_name\n",
    "SAVE_PATH        = root + \"models/\" + short_comp_name + '/'\n",
    "SUBMISSIONS_PATH = root + \"submissions/\" + short_comp_name + '/'\n",
    "SRC_PATH         = root + \"code/kaggle_salt/code_gazay/\"\n",
    "\n",
    "COMP_SRC_PATH    = SRC_PATH + short_comp_name\n",
    "LENIN_SRC_PATH   = SRC_PATH + \"lenin\"\n",
    "SCRIPTS_PATH     = SRC_PATH + \"scripts\"\n",
    "\n",
    "ARCH_NAME = 'albunet34'\n",
    "\n",
    "# Debug\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# stdlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "# Lenin straight from repo\n",
    "sys.path.insert(0, LENIN_SRC_PATH)\n",
    "import lenin\n",
    "from lenin import test\n",
    "from lenin.datasets.salt import Dataset\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Plots in IPython mode\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Utils from competition directory\n",
    "sys.path.insert(0, COMP_SRC_PATH)\n",
    "from src.utils import id_generator\n",
    "from src.utils.metrics import old_iou_lb, old_iou_numpy, intersection_over_union, intersection_over_union_thresholds\n",
    "from src.utils.split_data import split_data\n",
    "from src.utils.randomization import set_random_seed, base_model_name\n",
    "from src.utils.train_wrapper import Trainer\n",
    "from src.utils.plot import show_images\n",
    "from src.utils.rle import rle_encoding\n",
    "\n",
    "# Model architecture\n",
    "from src.models.albunet34 import new_model, load_model\n",
    "\n",
    "# Images resizing\n",
    "from src.utils.processors import Resizers, Processors\n",
    "\n",
    "# Data splitting\n",
    "from src.utils.split_data import split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomization (setting random seed and model name)\n",
    "global RANDOM_SEED\n",
    "RANDOM_SEED = set_random_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(DATASET_PATH)\n",
    "procs = Processors(aug_type='resize_pad_crop')\n",
    "dataset.preprocessors = procs.pre()\n",
    "dataset.postprocessors = procs.post()\n",
    "#dataset.train = dataset.train[0:500]\n",
    "#show_images(dataset)\n",
    "downsample_fn = procs.resizers.downsampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INFERENCE_ONLY:\n",
    "    MODEL_NAME = base_model_name(ARCH_NAME)\n",
    "    print(MODEL_NAME)\n",
    "    \n",
    "    import torchbearer\n",
    "    from torchbearer import metrics\n",
    "    from lenin import train\n",
    "\n",
    "    sys.path.insert(0, COMP_SRC_PATH)\n",
    "    from src.utils import id_generator\n",
    "    from src.utils.metrics import intersection_over_union, intersection_over_union_thresholds\n",
    "    from src.utils.split_data import split_data, cross_validate\n",
    "    from src.utils.randomization import set_random_seed, base_model_name\n",
    "    from src.utils.train_wrapper import Trainer\n",
    "\n",
    "    def get_iou_vector(y_t, y_p):\n",
    "        batch_size = y_t.shape[0]\n",
    "        metric = []\n",
    "        for batch in range(batch_size):\n",
    "            t, p = y_t[batch]>0, y_p[batch]>0\n",
    "\n",
    "            intersection = np.logical_and(t, p)\n",
    "            union = np.logical_or(t, p)\n",
    "            iou = (np.sum(intersection > 0) + 1e-10 )/ (np.sum(union > 0) + 1e-10)\n",
    "            thresholds = np.arange(0.5, 1, 0.05)\n",
    "            s = []\n",
    "            for thresh in thresholds:\n",
    "                s.append(iou > thresh)\n",
    "            metric.append(np.mean(s))\n",
    "\n",
    "        return np.mean(metric)\n",
    "\n",
    "    # Metric\n",
    "    @metrics.default_for_key('acc')\n",
    "    @metrics.running_mean\n",
    "    @metrics.std\n",
    "    @metrics.mean\n",
    "    @metrics.lambda_metric('acc', on_epoch=False)\n",
    "    def iou_pytorch(y_pred: torch.Tensor, y_true: torch.Tensor, prob_thres=0):\n",
    "        batch_size = y_true.shape[0]\n",
    "\n",
    "        metric = torch.tensor([]).float().cuda()\n",
    "        for batch in range(batch_size):\n",
    "            t, p = y_true[batch]>0, y_pred[batch]>0\n",
    "\n",
    "            intersection = (t & p)\n",
    "            union = (t | p)\n",
    "            iou = (torch.sum(intersection > 0).float() + 1e-10 ) / (torch.sum(union > 0).float() + 1e-10)\n",
    "            thresholds = torch.arange(0.5, 1, 0.05).float().cuda()\n",
    "            s = torch.tensor([]).float().cuda()\n",
    "            for thresh in thresholds:\n",
    "                s = torch.cat((s, (iou > thresh).float().unsqueeze(0)))\n",
    "            metric = torch.cat((metric, torch.mean(s).unsqueeze(0)))\n",
    "\n",
    "        return torch.mean(metric)\n",
    "    \n",
    "    # Hyperparams\n",
    "    stratify = True\n",
    "    val_size = 0.10\n",
    "    orig_options = {\n",
    "        'random_seed': RANDOM_SEED,\n",
    "        # For debug purposes set num workers to 0\n",
    "        'preload': { 'num_workers': 0 }, # 'pin_memory': True, 'worker_init_fn': _init_fn },\n",
    "\n",
    "        'augment': { ('image', 'mask'): [\n",
    "            {'type': 'HorizontalFlip'},\n",
    "            {'type': 'ShiftScaleRotate'},\n",
    "            {'type': 'Blur'},\n",
    "            {'type': 'GaussNoise' },\n",
    "            #{'type': 'RandomGamma' },\n",
    "            #{'type': 'OpticalDistortion' },\n",
    "            #{'type': 'GridDistortion' },\n",
    "            #{'type': 'ElasticTransform' },\n",
    "            #{'type': 'HueSaturationValue' },\n",
    "            #{'type': 'RandomBrightness' },\n",
    "            #{'type': 'RandomContrast' },\n",
    "            #{'type': 'MotionBlur' },\n",
    "            #{'type': 'MedianBlur' },\n",
    "            #{'type': 'CLAHE' },\n",
    "            #{'type': 'JpegCompression' },\n",
    "        ]},\n",
    "        'batch_size': 96,\n",
    "        'optimizer': ('adam', { 'lr': 1e-4 }),\n",
    "        'epochs': 10,\n",
    "        'loss': 'bce_xloss',\n",
    "        'metrics': ['loss', 'acc'],\n",
    "        'val_size': val_size,\n",
    "        'stratify_split': stratify,\n",
    "        'split': split_data(dataset, RANDOM_SEED, stratify=stratify, val_size=val_size)\n",
    "    }\n",
    "\n",
    "    steps = [\n",
    "        {\n",
    "            'step': 0,\n",
    "        },\n",
    "        {\n",
    "            'step': 1,\n",
    "            'optimizer': ('adam', { 'lr': 1e-4 }),\n",
    "            'loss': 'lovasz',\n",
    "            'epochs': 100\n",
    "        },\n",
    "        {\n",
    "            'step': 2,\n",
    "            'optimizer': ('adam', { 'lr': 1e-5 }),\n",
    "            'loss': 'lovasz',\n",
    "            'epochs': 100\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    model_name = \"jupyter_\" + id_generator(MODEL_NAME, with_folds=False)\n",
    "    #model_base = '2018-10-09-162719_b3b5b9e_seed-4621_albunet34'\n",
    "    #model_name = 'st%i_' + model_base + '_--_{epoch:02d}_{val_loss:.4f}_{val_acc:.4f}.pt'\n",
    "\n",
    "    trainer = Trainer(save_path=SAVE_PATH,\n",
    "                      code_path=SCRIPTS_PATH,\n",
    "                      dataset=dataset,\n",
    "                      model_name=model_name,\n",
    "                      model_load_fn=load_model,\n",
    "                      model_new_fn=new_model,\n",
    "                      train_fn=train)\n",
    "\n",
    "    trainer.sequence(steps, **orig_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.cat((t, torch.tensor([1]).long()))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIR = subprocess.check_output(['ls', SAVE_PATH]).decode('utf8').split(\"\\n\")[-2] + '/'\n",
    "_DIR = 'folds_2018-10-10_1c1844b_seed-4621_albunet34/'\n",
    "\n",
    "!ls {SAVE_PATH}\n",
    "print('----')\n",
    "!ls {SAVE_PATH + _DIR}\n",
    "!cat {SAVE_PATH + _DIR + 'params_st0.json'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = SAVE_PATH + _DIR\n",
    "arrs = []\n",
    "for _d in subprocess.check_output(['ls', _dir]).decode('utf8').split(\"\\n\")[:4]:\n",
    "    _dir_d = _dir + _d + '/'\n",
    "    arr = next(x for x in subprocess.check_output(['ls', _dir_d]).decode('utf8').split(\"\\n\") if x.endswith('test_preds.npy'))\n",
    "    arrs.append(np.load(_dir_d + arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = arrs.pop(0)\n",
    "for arr in arrs:\n",
    "    predictions += arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "fold_predictions = copy.deepcopy(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best = 0\n",
    "binary_predictions = (predictions > threshold_best).astype(int)\n",
    "all_masks = rle_encoding(binary_predictions)\n",
    "model_name = 'folds_2018-10-10-223412_1c1844b_seed-4621_albunet34_thres0.pt'\n",
    "submit = pd.DataFrame([[rec.split('-')[1] for rec in dataset.test], all_masks]).T\n",
    "submit.columns = ['id', 'rle_mask']\n",
    "submission_path = SUBMISSIONS_PATH + model_name.split('/')[-1].replace('.pt', '.csv')\n",
    "submit.to_csv(submission_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = subprocess.check_output(['ls', SAVE_PATH + _DIR]).decode('utf8').split(\"\\n\")[-2]\n",
    "model_name = 'st0_2018-10-10-045239_1f5e39f_seed-4621_albunet34_--_53_0.0247_0.8361.pt'\n",
    "acc = model_name.split('_')[-1].split('.')[1]\n",
    "print(\"Accuracy: \" + '.'.join([acc[:2], acc[2:]]))\n",
    "\n",
    "model = load_model(SAVE_PATH + _DIR + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_best = 0\n",
    "\n",
    "# МЕТРИКУ ЗАПИЛИ\n",
    "seed = int(model_name.split('seed-')[-1].split('_')[0])\n",
    "RANDOM_SEED = set_random_seed(seed)\n",
    "val_size = 0.1\n",
    "stratify_split = True\n",
    "\n",
    "train, val = split_data(dataset, random_seed=seed, val_size=val_size, stratify=stratify_split)\n",
    "val_dataset = Dataset(DATASET_PATH)\n",
    "val_dataset.preprocessors = procs.pre()\n",
    "val_dataset.postprocessors = procs.post()\n",
    "val_dataset.test = val\n",
    "val_preds = test(model, val_dataset, batch_size=24)\n",
    "val_preds = np.array([downsample_fn(pred) for pred in val_preds.data.cpu().numpy()[:, 0, :, :]])\n",
    "val_truth = np.array([dataset.mask(record) for record in val_dataset.test])\n",
    "assert val_preds.shape == val_truth.shape\n",
    "\n",
    "## Scoring for last model, choose threshold by validation data \n",
    "thresholds = np.linspace(-2, 2, 100)\n",
    "# Reverse sigmoid function: Use code below because the  sigmoid activation was removed\n",
    "#thresholds = np.log(thresholds_ori/(1-thresholds_ori))\n",
    "\n",
    "ious = np.array([iou_numpy(val_preds, val_truth, prob_thres=threshold) for threshold in tqdm_notebook(thresholds)])\n",
    "print(ious)\n",
    "\n",
    "# instead of using default 0 as threshold, use validation data to find the best threshold.\n",
    "threshold_best_index = np.argmax(ious) \n",
    "iou_best = ious[threshold_best_index]\n",
    "threshold_best = thresholds[threshold_best_index]\n",
    "\n",
    "plt.plot(thresholds, ious)\n",
    "plt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"IoU\")\n",
    "plt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(dataset, unprocess_fn=downsample_fn, model=model, threshold=threshold_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half1 = dataset.test[:9000]\n",
    "half2 = dataset.test[9000:]\n",
    "dataset.test = half1\n",
    "predictions = test(model, dataset, batch_size=8)\n",
    "predictions = np.array([downsample_fn(pred) for pred in predictions.data.cpu().numpy()[:, 0, :, :]])\n",
    "binary_predictions1 = (predictions > threshold_best).astype(int)\n",
    "predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.test = half2\n",
    "predictions = test(model, dataset, batch_size=8)\n",
    "predictions = np.array([downsample_fn(pred) for pred in predictions.data.cpu().numpy()[:, 0, :, :]])\n",
    "binary_predictions2 = (predictions > threshold_best).astype(int)\n",
    "predictions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = np.concatenate((binary_predictions1, binary_predictions2))\n",
    "dataset.test = np.concatenate((half1, half2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks = rle_encoding(binary_predictions)\n",
    "\n",
    "submit = pd.DataFrame([[rec.split('-')[1] for rec in dataset.test], all_masks]).T\n",
    "submit.columns = ['id', 'rle_mask']\n",
    "submission_path = SUBMISSIONS_PATH + model_name.split('/')[-1].replace('.pt', '.csv')\n",
    "submit.to_csv(submission_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = submission_path.replace(':', '\\:').replace('(', '\\(').replace(')', '\\)')\n",
    "!kaggle c submit -f {m} -m '{submission_path}' -c tgs-salt-identification-challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
